# -*- coding: utf-8 -*-
"""finalAi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UR--LqOcjTm__ZHrqBdthTHJbprMVYko
"""

import pandas as pd
from pandas import DataFrame
import sklearn
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer 
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import opinion_lexicon
from sklearn import tree
import numpy as np
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
from sklearn import tree
from sklearn.metrics import f1_score
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score

nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('opinion_lexicon')
nltk.download('averaged_perceptron_tagger')
nltk.download('vader_lexicon')
#read csv file
data = pd.read_csv("train.csv", low_memory=False)
#print(data)
#data.Type.value_counts()
#show the first 80% of the data 
data = data.head(9575)
data['Id'] = data['Id'].fillna("-1")
data['Tweet'] = data['Tweet'].fillna("-1")
data['following'] = data['following'].fillna(-1)
data['followers'] = data['followers'].fillna(-1)
data['actions'] = data['actions'].fillna(-1)
data['is_retweet'] = data['is_retweet'].fillna(-1)
data['location'] = data['location'].fillna("-1")

"""Text preprocessing

"""

TokenList =[]
for column in data['Tweet']:
   text_tokens = word_tokenize(column)
   TokenList.append(text_tokens)
len(TokenList)
TokenList

from google.colab import drive
drive.mount('/content/drive')

"""Remove stop words"""

# Commented out IPython magic to ensure Python compatibility.
def removeSpecialCharacter(data):
    data = re.sub("[^a-zA-Z0-9 ]", "", data)
    return data
print(data['Tweet'])
# %load_ext google.colab.data_table

def stopWordRemove(TokenList):
  tokens_without_sw=[]
  sw = set(stopwords.words('english'))
  for i in range(0,len(TokenList)):
    words_list = []
    for word in TokenList[i]:
      # print(word)
      if not (word in sw):
        words_list.append(word)
    tokens_without_sw.append(words_list)
  return tokens_without_sw
tokens_without_sw = stopWordRemove(TokenList)
tokens_without_sw

"""lemmatization"""

def lemmatization(tokens_without_sw):
  lmtz_list=[]
  lemmatizer = WordNetLemmatizer()
  for words_list in tokens_without_sw:
    lmtz_words_list = []
    for word in words_list:
      lmtz_words_list.append(lemmatizer.lemmatize(word))
      print("Lemma for {} is {}".format(word, lemmatizer.lemmatize(word)))
    lmtz_list.append(lmtz_words_list)

  return  lmtz_list
lmtz_list=lemmatization(tokens_without_sw)
lmtz_list #without sw and lemmatized

"""Feature Extraction"""

corpus = []
#join the words to use it in vectorizer 

for list_of_words in lmtz_list:
  corpus.append((" ").join(list_of_words))
corpus

"""# New Section"""

vectorizer = CountVectorizer(max_df=0.90, min_df=0.10)
X = vectorizer.fit_transform(corpus)
print(vectorizer.get_feature_names())
X = X.toarray()
# print(X.toarray()

def numofhashtags(lmtz_list):

  count = 0;  
  numofhash = []
  marks = ["#"]
  for listt in lmtz_list: 
      count=0
      for word in listt:   
          
         if word in marks: 
            count = count + 1  
      numofhash.append(count)
  return(numofhash)
numofhash=numofhashtags(lmtz_list)
numofhash

"""Number of Mentions @

"""

def numofmentions(lmtz_list):
  count = 0;  
  numofmention = []
  marks = ["@"]
  for listt in lmtz_list: 
      count=0
      for word in listt:   
         #Checks whether given character is @ 
         if word in marks: 
            count = count + 1  
      numofmention.append(count)
  return(numofmention)
numofmention=numofmentions(lmtz_list)
numofmention

# checks if the content has URL using regex
import re
def hasURL(corpus):
  numofurls = []
  count=0
  p1=[]
  #r = re.compile(".*http.*")
  for list4 in corpus :
    count=0
    #if (re.findall('http', list4)):
    p=re.findall('http', list4)
   # print(type(p))
    count = count+len(p)

   # p= [tuple(j for j in i if j)[-1] for i in p]
    
    #
    numofurls.append(count)
  return numofurls
numofurls =hasURL(corpus)
numofurls
      # regex = r"(?i)\b((?:https?://|www\d{0,3}[.]|[a-z0-9.\-]+[.][a-z]{2,4}/)(?:[^\s()<>]+|\(([^\s()<>]+|(\([^\s()<>]+\)))*\))+(?:\(([^\s()<>]+|(\([^\s()<>]+\)))*\)|[^\s`!()\[\]{};:'\".,<>?«»“”‘’]))"
   # url = re.findall(r, corpus)
    #return [x[0] for x in url]

featureData = data[ ['location','Type'] ]

def tweetContentLength(str):
    return len(str)

featureData['Length'] = data.apply( lambda row: tweetContentLength(row['location']), axis = 1 )
featureData

e=data['Id'].tolist()
e

def numofretweet():
  g=data['is_retweet'].tolist()
  return g 
g=numofretweet()

g

def Ratio1():
  followers=data['followers'].tolist()
  following=data['following'].tolist()
  Ratio = []
  
  list_3 = [followers[i] +following[i] for i in range(0,9574+1)]

  for i in range(0,9574+1):
    if (list_3[i]==0):
      Ratio.append(-1)
    else:
      Ratio.append(followers[i] / list_3[i])
  return  Ratio
Ratio=Ratio1()


Ratio



"""**Classification**"""

import numpy as np
X = np.array(X)
X = np.c_[X, data['Id']]
#X = np.c_[X, data['followers']]
#X = np.c_[X, data['following']]
X = np.c_[X, data['is_retweet']]
X = np.c_[X , numofmention]
X= np.c_[X, Ratio]
X= np.c_[X, numofhash]
X = np.c_[X, numofurls]

len(X)

from sklearn.model_selection import train_test_split
X=X.tolist()
Y22 = data['Type'].map({'Quality': 1,'Spam': 0}).astype(int)
Y=Y22.tolist()
X_train, X_test, Y_train, Y_test = train_test_split( X, Y, test_size=0.30 )

X_train = np.nan_to_num(np.array(X_train))
X_test = np.nan_to_num(np.array(X_test))
Y_train = np.nan_to_num(np.array(Y_train))
Y_test = np.nan_to_num(np.array(Y_test))

X_train=np.array(X_train).astype(np.float)
X_test=np.array(X_test).astype(np.float)
Y_train=np.array(Y_train).astype(np.float)
Y_test=np.array(Y_test).astype(np.float)

from sklearn import tree

clf = tree.DecisionTreeClassifier()
clf = clf.fit(X_train, Y_train)

count=0
predicted_out=clf.predict(X_test)
print(accuracy_score(Y_test, predicted_out),f1_score(Y_test, predicted_out),precision_score(Y_test, predicted_out), recall_score(Y_test, predicted_out))

from sklearn.naive_bayes import GaussianNB

gnb = GaussianNB()
gnb_model = gnb.fit(X_train, Y_train)
gnb_model

count=0
predicted_out=gnb_model.predict(X_test)
print(accuracy_score(Y_test, predicted_out),f1_score(Y_test, predicted_out),precision_score(Y_test, predicted_out), recall_score(Y_test, predicted_out))

from sklearn.ensemble import RandomForestClassifier
rfor = RandomForestClassifier()
rfor.fit(X_train, Y_train)

count=0
predicted_out=rfor.predict(X_test)
print(accuracy_score(Y_test, predicted_out),f1_score(Y_test, predicted_out),precision_score(Y_test, predicted_out), recall_score(Y_test, predicted_out))

from collections import Counter
from imblearn.over_sampling import SMOTE

SMOTE = SMOTE()
X_train_SMOTE, Y_train_SMOTE = SMOTE.fit_resample(X_train, Y_train)

print("befor balncing: "+str(Counter(Y_train)),"after balancing: "+ str(Counter(Y_train_SMOTE)))

from sklearn import tree

DT = tree.DecisionTreeClassifier()
DT = DT.fit(X_train_SMOTE, Y_train_SMOTE)
gnb = GaussianNB()
gnb_model = gnb.fit(X_train_SMOTE, Y_train_SMOTE)
rfor = RandomForestClassifier()
rfor.fit(X_train_SMOTE, Y_train_SMOTE)

DT_predicted=clf.predict(X_test)
print(accuracy_score(Y_test, DT_predicted),f1_score(Y_test, DT_predicted),precision_score(Y_test, DT_predicted), recall_score(Y_test, DT_predicted))

predicted_out=gnb_model.predict(X_test)
print(accuracy_score(Y_test, predicted_out),f1_score(Y_test, predicted_out),precision_score(Y_test, predicted_out), recall_score(Y_test, predicted_out))

predicted_out1=rfor.predict(X_test)
print(accuracy_score(Y_test, predicted_out1),f1_score(Y_test, predicted_out),precision_score(Y_test, predicted_out1), recall_score(Y_test, predicted_out1))

